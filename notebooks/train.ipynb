{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1MUDw5zMeJVReN5uuGyYppRxwqKx-rf_t","authorship_tag":"ABX9TyPgVj7fm9ifwmhyZBURDLXD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"fZ_WBVTEZUuq","executionInfo":{"status":"ok","timestamp":1736304451470,"user_tz":-420,"elapsed":1482,"user":{"displayName":"Quang Hoàng Nhật","userId":"13854286472866755146"}},"outputId":"69256218-a777-457b-e1da-e6e361f8c80b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Current working directory:\n","/content/drive/MyDrive/Documents/project/deep-learning/depth-estimation\n","\n","Directory contents:\n","\u001b[0m\u001b[01;34mcheckpoint\u001b[0m/  \u001b[01;34mdata\u001b[0m/    \u001b[01;34mmodel\u001b[0m/      \u001b[01;34m__pycache__\u001b[0m/      \u001b[01;34mtests\u001b[0m/      \u001b[01;34mutils\u001b[0m/\n","\u001b[01;34mconfigs\u001b[0m/     main.py  \u001b[01;34mnotebooks\u001b[0m/  requirements.txt  trainer.py  \u001b[01;34mwandb\u001b[0m/\n"]}],"source":["# @title Change working directory\n","print(\"Current working directory:\")\n","%cd /content/drive/MyDrive/Documents/project/deep-learning/depth-estimation\n","print(\"\\nDirectory contents:\")\n","%ls"]},{"cell_type":"code","source":["# @title Install dependencies\n","!cat requirements.txt | xargs -n 1 pip install > /dev/null"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"XxnpK-9NaEGl","executionInfo":{"status":"ok","timestamp":1736304480817,"user_tz":-420,"elapsed":26529,"user":{"displayName":"Quang Hoàng Nhật","userId":"13854286472866755146"}},"outputId":"268cdf73-2abf-43ff-ebf0-b7b49d738543"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.24.1 which is incompatible.\n","albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.24.1 which is incompatible.\n","pymc 5.19.1 requires numpy>=1.25.0, but you have numpy 1.24.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["# @title Log in to Weight & Bias\n","!wandb login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"HbPgUiOEcC3c","executionInfo":{"status":"ok","timestamp":1736304516960,"user_tz":-420,"elapsed":36149,"user":{"displayName":"Quang Hoàng Nhật","userId":"13854286472866755146"}},"outputId":"dbe01afa-e79c-4b4a-b45c-7a342a9f1a17"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}]},{"cell_type":"code","source":["# @title Training command\n","!python main.py --mode 0 --config \"configs/monodepth_test.yaml\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"EH93ARWLipbO","outputId":"df887a73-89a5-44c2-8ef6-ee8d530ed7f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhnquang-cs\u001b[0m (\u001b[33mhnquang-cs-ho-chi-minh-city-university-of-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/Documents/project/deep-learning/depth-estimation/wandb/run-20250108_032616-sccf71sx\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtest\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/hnquang-cs-ho-chi-minh-city-university-of-technology/monodepth\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/hnquang-cs-ho-chi-minh-city-university-of-technology/monodepth/runs/sccf71sx\u001b[0m\n","===============================================================================================\n","Layer (type:depth-idx)                        Input Shape               Output Shape\n","===============================================================================================\n","DepthEstimationModel                          [8, 3, 192, 640]          [8, 1, 48, 160]\n","├─ResNet: 1-1                                 [8, 3, 192, 640]          [8, 3, 192, 640]\n","│    └─Sequential: 2-1                        [8, 3, 192, 640]          [8, 64, 96, 320]\n","│    │    └─Conv2d: 3-1                       [8, 3, 192, 640]          [8, 64, 96, 320]\n","│    │    └─BatchNorm2d: 3-2                  [8, 64, 96, 320]          [8, 64, 96, 320]\n","│    │    └─ReLU: 3-3                         [8, 64, 96, 320]          [8, 64, 96, 320]\n","│    └─Sequential: 2-2                        [8, 64, 96, 320]          [8, 256, 48, 160]\n","│    │    └─MaxPool2d: 3-4                    [8, 64, 96, 320]          [8, 64, 48, 160]\n","│    │    └─BottleNeck: 3-5                   [8, 64, 48, 160]          [8, 256, 48, 160]\n","│    │    └─BottleNeck: 3-6                   [8, 256, 48, 160]         [8, 256, 48, 160]\n","│    │    └─BottleNeck: 3-7                   [8, 256, 48, 160]         [8, 256, 48, 160]\n","│    └─Sequential: 2-3                        [8, 256, 48, 160]         [8, 512, 24, 80]\n","│    │    └─BottleNeck: 3-8                   [8, 256, 48, 160]         [8, 512, 24, 80]\n","│    │    └─BottleNeck: 3-9                   [8, 512, 24, 80]          [8, 512, 24, 80]\n","│    │    └─BottleNeck: 3-10                  [8, 512, 24, 80]          [8, 512, 24, 80]\n","│    │    └─BottleNeck: 3-11                  [8, 512, 24, 80]          [8, 512, 24, 80]\n","│    └─Sequential: 2-4                        [8, 512, 24, 80]          [8, 1024, 12, 40]\n","│    │    └─BottleNeck: 3-12                  [8, 512, 24, 80]          [8, 1024, 12, 40]\n","│    │    └─BottleNeck: 3-13                  [8, 1024, 12, 40]         [8, 1024, 12, 40]\n","│    │    └─BottleNeck: 3-14                  [8, 1024, 12, 40]         [8, 1024, 12, 40]\n","│    │    └─BottleNeck: 3-15                  [8, 1024, 12, 40]         [8, 1024, 12, 40]\n","│    │    └─BottleNeck: 3-16                  [8, 1024, 12, 40]         [8, 1024, 12, 40]\n","│    │    └─BottleNeck: 3-17                  [8, 1024, 12, 40]         [8, 1024, 12, 40]\n","│    └─Sequential: 2-5                        [8, 1024, 12, 40]         [8, 2048, 6, 20]\n","│    │    └─BottleNeck: 3-18                  [8, 1024, 12, 40]         [8, 2048, 6, 20]\n","│    │    └─BottleNeck: 3-19                  [8, 2048, 6, 20]          [8, 2048, 6, 20]\n","│    │    └─BottleNeck: 3-20                  [8, 2048, 6, 20]          [8, 2048, 6, 20]\n","├─Decoder: 1-2                                [8, 3, 192, 640]          [8, 1, 48, 160]\n","│    └─UpConv: 2-6                            [8, 2048, 6, 20]          [8, 512, 12, 40]\n","│    │    └─Conv2d: 3-21                      [8, 2048, 12, 40]         [8, 512, 12, 40]\n","│    │    └─Sequential: 3-22                  [8, 2048, 12, 40]         [8, 512, 12, 40]\n","│    │    └─Sequential: 3-23                  [8, 512, 12, 40]          [8, 512, 12, 40]\n","│    │    └─ReLU: 3-24                        [8, 512, 12, 40]          [8, 512, 12, 40]\n","│    └─Sequential: 2-7                        [8, 1536, 12, 40]         [8, 512, 12, 40]\n","│    │    └─Conv2d: 3-25                      [8, 1536, 12, 40]         [8, 512, 12, 40]\n","│    │    └─BatchNorm2d: 3-26                 [8, 512, 12, 40]          [8, 512, 12, 40]\n","│    │    └─ReLU: 3-27                        [8, 512, 12, 40]          [8, 512, 12, 40]\n","│    └─UpConv: 2-8                            [8, 512, 12, 40]          [8, 256, 24, 80]\n","│    │    └─Conv2d: 3-28                      [8, 512, 24, 80]          [8, 256, 24, 80]\n","│    │    └─Sequential: 3-29                  [8, 512, 24, 80]          [8, 256, 24, 80]\n","│    │    └─Sequential: 3-30                  [8, 256, 24, 80]          [8, 256, 24, 80]\n","│    │    └─ReLU: 3-31                        [8, 256, 24, 80]          [8, 256, 24, 80]\n","│    └─Sequential: 2-9                        [8, 768, 24, 80]          [8, 256, 24, 80]\n","│    │    └─Conv2d: 3-32                      [8, 768, 24, 80]          [8, 256, 24, 80]\n","│    │    └─BatchNorm2d: 3-33                 [8, 256, 24, 80]          [8, 256, 24, 80]\n","│    │    └─ReLU: 3-34                        [8, 256, 24, 80]          [8, 256, 24, 80]\n","│    └─UpConv: 2-10                           [8, 256, 24, 80]          [8, 128, 48, 160]\n","│    │    └─Conv2d: 3-35                      [8, 256, 48, 160]         [8, 128, 48, 160]\n","│    │    └─Sequential: 3-36                  [8, 256, 48, 160]         [8, 128, 48, 160]\n","│    │    └─Sequential: 3-37                  [8, 128, 48, 160]         [8, 128, 48, 160]\n","│    │    └─ReLU: 3-38                        [8, 128, 48, 160]         [8, 128, 48, 160]\n","│    └─Sequential: 2-11                       [8, 384, 48, 160]         [8, 128, 48, 160]\n","│    │    └─Conv2d: 3-39                      [8, 384, 48, 160]         [8, 128, 48, 160]\n","│    │    └─BatchNorm2d: 3-40                 [8, 128, 48, 160]         [8, 128, 48, 160]\n","│    │    └─ReLU: 3-41                        [8, 128, 48, 160]         [8, 128, 48, 160]\n","│    └─Sequential: 2-12                       [8, 128, 48, 160]         [8, 1, 48, 160]\n","│    │    └─Conv2d: 3-42                      [8, 128, 48, 160]         [8, 1, 48, 160]\n","│    │    └─Sigmoid: 3-43                     [8, 1, 48, 160]           [8, 1, 48, 160]\n","│    └─UpConv: 2-13                           [8, 128, 48, 160]         [8, 64, 96, 320]\n","│    │    └─Conv2d: 3-44                      [8, 128, 96, 320]         [8, 64, 96, 320]\n","│    │    └─Sequential: 3-45                  [8, 128, 96, 320]         [8, 64, 96, 320]\n","│    │    └─Sequential: 3-46                  [8, 64, 96, 320]          [8, 64, 96, 320]\n","│    │    └─ReLU: 3-47                        [8, 64, 96, 320]          [8, 64, 96, 320]\n","│    └─Sequential: 2-14                       [8, 129, 96, 320]         [8, 64, 96, 320]\n","│    │    └─Conv2d: 3-48                      [8, 129, 96, 320]         [8, 64, 96, 320]\n","│    │    └─BatchNorm2d: 3-49                 [8, 64, 96, 320]          [8, 64, 96, 320]\n","│    │    └─ReLU: 3-50                        [8, 64, 96, 320]          [8, 64, 96, 320]\n","│    └─Sequential: 2-15                       [8, 64, 96, 320]          [8, 1, 96, 320]\n","│    │    └─Conv2d: 3-51                      [8, 64, 96, 320]          [8, 1, 96, 320]\n","│    │    └─Sigmoid: 3-52                     [8, 1, 96, 320]           [8, 1, 96, 320]\n","│    └─UpConv: 2-16                           [8, 64, 96, 320]          [8, 32, 192, 640]\n","│    │    └─Conv2d: 3-53                      [8, 64, 192, 640]         [8, 32, 192, 640]\n","│    │    └─Sequential: 3-54                  [8, 64, 192, 640]         [8, 32, 192, 640]\n","│    │    └─Sequential: 3-55                  [8, 32, 192, 640]         [8, 32, 192, 640]\n","│    │    └─ReLU: 3-56                        [8, 32, 192, 640]         [8, 32, 192, 640]\n","│    └─UpConv: 2-17                           [8, 64, 96, 320]          [8, 32, 192, 640]\n","│    │    └─Conv2d: 3-57                      [8, 64, 192, 640]         [8, 32, 192, 640]\n","│    │    └─Sequential: 3-58                  [8, 64, 192, 640]         [8, 32, 192, 640]\n","│    │    └─Sequential: 3-59                  [8, 32, 192, 640]         [8, 32, 192, 640]\n","│    │    └─ReLU: 3-60                        [8, 32, 192, 640]         [8, 32, 192, 640]\n","│    └─Sequential: 2-18                       [8, 65, 192, 640]         [8, 32, 192, 640]\n","│    │    └─Conv2d: 3-61                      [8, 65, 192, 640]         [8, 32, 192, 640]\n","│    │    └─BatchNorm2d: 3-62                 [8, 32, 192, 640]         [8, 32, 192, 640]\n","│    │    └─ReLU: 3-63                        [8, 32, 192, 640]         [8, 32, 192, 640]\n","│    └─Sequential: 2-19                       [8, 32, 192, 640]         [8, 1, 192, 640]\n","│    │    └─Conv2d: 3-64                      [8, 32, 192, 640]         [8, 1, 192, 640]\n","│    │    └─Sigmoid: 3-65                     [8, 1, 192, 640]          [8, 1, 192, 640]\n","===============================================================================================\n","Total params: 72,708,835\n","Trainable params: 72,708,835\n","Non-trainable params: 0\n","Total mult-adds (G): 576.15\n","===============================================================================================\n","Input size (MB): 11.80\n","Forward/backward pass size (MB): 9589.06\n","Params size (MB): 290.84\n","Estimated Total Size (MB): 9891.70\n","===============================================================================================\n","======== BEGIN FITTING STAGE ========\n","==> Epoch [0/50]\n","Training: 100% 62/62 [01:15<00:00,  1.22s/batch]\n","Validating: 100% 20/20 [00:20<00:00,  1.05s/batch]\n","==> Epoch [1/50]\n","Training: 100% 62/62 [01:07<00:00,  1.08s/batch]\n","Validating: 100% 20/20 [00:19<00:00,  1.05batch/s]\n","==> Epoch [2/50]\n","Training: 100% 62/62 [01:08<00:00,  1.10s/batch]\n","Validating: 100% 20/20 [00:19<00:00,  1.01batch/s]\n","==> Epoch [3/50]\n","Training: 100% 62/62 [01:07<00:00,  1.10s/batch]\n","Validating: 100% 20/20 [00:19<00:00,  1.05batch/s]\n","==> Epoch [4/50]\n","Training: 100% 62/62 [01:08<00:00,  1.11s/batch]\n","Validating: 100% 20/20 [00:19<00:00,  1.03batch/s]\n","==> Epoch [5/50]\n","Training: 100% 62/62 [01:07<00:00,  1.09s/batch]\n","Validating: 100% 20/20 [00:20<00:00,  1.05s/batch]\n","==> Epoch [6/50]\n","Training: 100% 62/62 [01:04<00:00,  1.04s/batch]\n","Validating: 100% 20/20 [00:20<00:00,  1.00s/batch]\n","==> Epoch [7/50]\n","Training: 100% 62/62 [01:05<00:00,  1.05s/batch]\n","Validating: 100% 20/20 [00:20<00:00,  1.01s/batch]\n","==> Epoch [8/50]\n","Training: 100% 62/62 [01:04<00:00,  1.04s/batch]\n","Validating: 100% 20/20 [00:19<00:00,  1.03batch/s]\n","==> Epoch [9/50]\n","Training: 100% 62/62 [01:07<00:00,  1.09s/batch]\n","Validating: 100% 20/20 [00:19<00:00,  1.02batch/s]\n","==> Epoch [10/50]\n","Training: 100% 62/62 [01:06<00:00,  1.08s/batch]\n","Validating: 100% 20/20 [00:19<00:00,  1.03batch/s]\n","==> Epoch [11/50]\n","Training: 100% 62/62 [01:08<00:00,  1.10s/batch]\n","Validating: 100% 20/20 [00:19<00:00,  1.02batch/s]\n","==> Epoch [12/50]\n","Training: 100% 62/62 [01:10<00:00,  1.13s/batch]\n","Validating: 100% 20/20 [00:19<00:00,  1.04batch/s]\n","==> Epoch [13/50]\n","Training: 100% 62/62 [01:04<00:00,  1.04s/batch]\n","Validating: 100% 20/20 [00:19<00:00,  1.05batch/s]\n","==> Epoch [14/50]\n","Training: 100% 62/62 [01:09<00:00,  1.11s/batch]\n","Validating: 100% 20/20 [00:19<00:00,  1.04batch/s]\n","==> Epoch [15/50]\n","Training: 100% 62/62 [01:05<00:00,  1.05s/batch]\n","Validating: 100% 20/20 [00:19<00:00,  1.05batch/s]\n","==> Epoch [16/50]\n","Training: 100% 62/62 [01:08<00:00,  1.10s/batch]\n","Validating: 100% 20/20 [00:19<00:00,  1.00batch/s]\n","==> Epoch [17/50]\n","Training: 100% 62/62 [01:06<00:00,  1.08s/batch]\n","Validating: 100% 20/20 [00:19<00:00,  1.00batch/s]\n","==> Epoch [18/50]\n","Training: 100% 62/62 [01:07<00:00,  1.09s/batch]\n","Validating: 100% 20/20 [00:20<00:00,  1.03s/batch]\n","==> Epoch [19/50]\n","Training: 100% 62/62 [01:05<00:00,  1.05s/batch]\n","Validating: 100% 20/20 [00:21<00:00,  1.05s/batch]\n","==> Epoch [20/50]\n","Training: 100% 62/62 [01:06<00:00,  1.07s/batch]\n","Validating: 100% 20/20 [00:19<00:00,  1.00batch/s]\n","/content/drive/MyDrive/Documents/project/deep-learning/depth-estimation/trainer.py:111: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n","  fig, axs = plt.subplots(nrows=4, ncols=3, figsize=(30,9), constrained_layout=True)\n","==> Epoch [21/50]\n","Training: 100% 62/62 [01:05<00:00,  1.05s/batch]\n","Validating: 100% 20/20 [00:19<00:00,  1.01batch/s]\n","==> Epoch [22/50]\n","Training: 100% 62/62 [01:06<00:00,  1.08s/batch]\n","Validating: 100% 20/20 [00:19<00:00,  1.01batch/s]\n","==> Epoch [23/50]\n","Training: 100% 62/62 [01:05<00:00,  1.06s/batch]\n","Validating: 100% 20/20 [00:19<00:00,  1.03batch/s]\n","==> Epoch [24/50]\n","Training: 100% 62/62 [01:06<00:00,  1.07s/batch]\n","Validating: 100% 20/20 [00:20<00:00,  1.00s/batch]\n","==> Epoch [25/50]\n","Training: 100% 62/62 [01:05<00:00,  1.06s/batch]\n","Validating: 100% 20/20 [00:19<00:00,  1.04batch/s]\n","==> Epoch [26/50]\n","Training: 100% 62/62 [01:05<00:00,  1.06s/batch]\n","Validating: 100% 20/20 [00:20<00:00,  1.01s/batch]\n","==> Epoch [27/50]\n","Training: 100% 62/62 [01:04<00:00,  1.04s/batch]\n","Validating: 100% 20/20 [00:18<00:00,  1.06batch/s]\n","==> Epoch [28/50]\n","Training: 100% 62/62 [01:03<00:00,  1.03s/batch]\n","Validating: 100% 20/20 [00:18<00:00,  1.06batch/s]\n","==> Epoch [29/50]\n","Training: 100% 62/62 [01:05<00:00,  1.06s/batch]\n","Validating: 100% 20/20 [00:18<00:00,  1.05batch/s]\n","==> Epoch [30/50]\n","Training: 100% 62/62 [01:04<00:00,  1.04s/batch]\n","Validating: 100% 20/20 [00:18<00:00,  1.06batch/s]\n","==> Epoch [31/50]\n","Training: 100% 62/62 [01:04<00:00,  1.05s/batch]\n","Validating: 100% 20/20 [00:19<00:00,  1.04batch/s]\n","==> Epoch [32/50]\n","Training: 100% 62/62 [01:04<00:00,  1.03s/batch]\n","Validating: 100% 20/20 [00:19<00:00,  1.03batch/s]\n","==> Epoch [33/50]\n","Training: 100% 62/62 [01:04<00:00,  1.05s/batch]\n","Validating: 100% 20/20 [00:19<00:00,  1.01batch/s]\n","==> Epoch [34/50]\n","Training: 100% 62/62 [01:08<00:00,  1.11s/batch]\n","Validating: 100% 20/20 [00:19<00:00,  1.05batch/s]\n","==> Epoch [35/50]\n","Training: 100% 62/62 [01:04<00:00,  1.05s/batch]\n","Validating: 100% 20/20 [00:20<00:00,  1.02s/batch]\n","==> Epoch [36/50]\n","Training: 100% 62/62 [01:05<00:00,  1.06s/batch]\n","Validating: 100% 20/20 [00:19<00:00,  1.05batch/s]\n","==> Epoch [37/50]\n","Training: 100% 62/62 [01:05<00:00,  1.05s/batch]\n","Validating: 100% 20/20 [00:18<00:00,  1.05batch/s]\n","==> Epoch [38/50]\n","Training: 100% 62/62 [01:05<00:00,  1.05s/batch]\n","Validating: 100% 20/20 [00:19<00:00,  1.04batch/s]\n","==> Epoch [39/50]\n","Training: 100% 62/62 [01:07<00:00,  1.09s/batch]\n","Validating: 100% 20/20 [00:20<00:00,  1.00s/batch]\n","==> Epoch [40/50]\n","Training: 100% 62/62 [01:05<00:00,  1.05s/batch]\n","Validating:  55% 11/20 [00:10<00:08,  1.10batch/s]"]}]}]}